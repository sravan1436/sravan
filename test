import pytest
import json
import sys
import time
from datetime import datetime
from datetime import datetime as dt
import boto3
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
from awsglue.job import Job
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql.functions import lit, trim, col, unix_timestamp, udf, regexp_replace, to_utc_timestamp
from pyspark.sql.types import array, ArrayType, IntegerType, ShortType, DecimalType, StringType, StructType, \
    TimestampType

args = getResolvedOptions(sys.argv,
                          ['JOB_NAME', 'source_bucket', 'dest_bucket', 'source_prefix_spi', 'source_prefix_non_spi',
                           'dest_prefix_spi', 'dest_prefix_non_spi', 'eui_opt_config_bucket', 'rename_key',
                           'datatype_chng_key'])

JOB_NAME = 'euiingestion-etl-glue3-raw-to-conformed-test1'
source_bucket = 'sf-datalake1-test1-use1-raw'
dest_bucket = 'sf-datalake1-test1-use1-conformed'
source_prefix_spi = 'eui/secured/temp/'
source_prefix_non_spi = 'eui/temp/'
dest_prefix_spi = 'eui/secured/'
dest_prefix_non_spi = 'eui/'
eui_opt_config_bucket ='sf-pcingest-test1-use1-euiingestion-intermediary'
# Todo env variable
rename_key = 'policy_consolidation/eui_optimization_columnRename.json'
datatype_chng_key = 'policy_consolidation/eui_optimization_datatype_updates.json'
source_path = []
dest_dict = {}
config_rename_lst = []
config_datatype_lst = []

@pytest.fixture
def test_read_config_file(inBkt, inKey):
    global config_rename_lst
    global config_datatype_lst
    s3 = boto3.client('s3')
    config_obj = s3.get_object(
        Bucket=inBkt,
        Key=inKey)
    config_data = config_obj['Body'].read()
    config_data_json = json.loads(config_data)
    return config_data_json


@pytest.fixture
def test_craete_flat_of_changes(configdata, table, oper):
    config_flat_lst = []
    global config_rename_lst
    global config_datatype_lst
    try:
        for x, y in configdata[table.lower()].items():
            config_flat_lst.append([table.lower(), x.strip(), y.strip()])
    except KeyError:
        logger.info(f'{table.lower()} does not require any operations for {oper}')
    if oper == "rename":
        config_rename_lst = list(config_flat_lst)
        return config_rename_lst
    elif oper == "datatype":
        config_datatype_lst = list(config_flat_lst)
        return config_datatype_lst

@pytest.fixture
def test_tryconvert(value, default):
    try:
        return dt.strptime(value, '%Y-%m-%d-%H.%M.%S.%f')
    except (ValueError, TypeError):
        return dt.strptime(default, '%Y-%m-%d-%H.%M.%S.%f')

@pytest.fixture
def test_update_data_type(inDF, inColList, oper):
    """
    Agruments:   get input dataframe at table level ( ex  - addr_info),
    list element ex [ addr_info, colnane,target datatype]
    """
    outDF = inDF
    if oper == 'datatype':
        outDF = inDF.withColumn(inColList[1], trim(col(inColList[1])))
        if inColList[2] == "timestamp":
            outDF = outDF.withColumn(inColList[1],
                                     regexp_replace(inColList[1], '^(.{4})-(.{2})-(.{2})-(.{2})\\.(.{2})\\.(.{2})',
                                                    '$1-$2-$3 $4:$5:$6'))
            outDF = outDF.withColumn(inColList[1], to_utc_timestamp(col(inColList[1]), "UTC"))
        else:
            outDF = outDF.withColumn(inColList[1], outDF[inColList[1]].cast(inColList[2]))
        logger.info(f'{oper} changed for column {inColList[1]} to {inColList[2]}')
    elif oper == 'rename':
        outDF = inDF.withColumnRenamed(inColList[2], inColList[1])
        logger.info(f'{oper}ed column {inColList[2]} to {inColList[1]}')
    return outDF

@pytest.fixture
def test_drop_columns(inDyF, col1, col2):
    outDF = inDyF.toDF().drop(col1, col2)
    outDyF = DynamicFrame.fromDF(outDF, glueContext, "outDyF")
    return outDyF

@pytest.fixture
def test_create_source_dest_path_non_spi():
    s3_client = boto3.client('s3')
    result = s3_client.list_objects(Bucket=source_bucket, Prefix=source_prefix_non_spi, Delimiter='/')
    for prefixes in result.get('CommonPrefixes'):
        source_path.append(prefixes.get('Prefix'))
        dest_dict[prefixes.get('Prefix')] = (
            str(prefixes.get('Prefix')).replace(source_prefix_non_spi, dest_prefix_non_spi))
@pytest.fixture
def test_create_source_dest_path_spi():
    s3_client = boto3.client('s3')
    result = s3_client.list_objects(Bucket=source_bucket, Prefix=source_prefix_spi, Delimiter='/')
    for prefixes in result.get('CommonPrefixes'):
        source_path.append(prefixes.get('Prefix'))
        dest_dict[prefixes.get('Prefix')] = (str(prefixes.get('Prefix')).replace(source_prefix_spi, dest_prefix_spi))
@pytest.fixture
def test_stage_errors_count(etl_stage, error_count, job_name=JOB_NAME):
    if error_count > 0:
        raise Exception(
            f'Error encountered in {etl_stage} with error count as {error_count}..ETL Job was aborted, Investigate {job_name} glue job error Logs')
    pass
@pytest.fixture
def test_record_count(row_count, path_src):
    if row_count < 1:
        logger.info(
            f'{row_count} records read from {path_src},nothing to write commit job and  exit 0')
        glueJob.commit()
    else:
        logger.info(
            f'{row_count} records read from {path_src},continuing with next stage')
@pytest.fixture
def test_glue_read(src_path, subfolder_name, ip_format='parquet'):
    glue_read_input = glueContext.create_dynamic_frame_from_options(connection_type="s3",
                                                                    connection_options={"paths": [src_path]},
                                                                    format=ip_format,
                                                                    transformation_ctx="dy_read_input_" + str(
                                                                        subfolder_name))
    return glue_read_input
@pytest.fixture
def test_glue_write(dst_path, subfolder_name, op_format='parquet'):
    glue_write_output = glueContext.write_dynamic_frame_from_options(frame=dynamic_frame_output, connection_type='s3',
                                                                     connection_options={"path": dst_path,
                                                                                         "partitionKeys": [
                                                                                             "ingestion_year",
                                                                                             "ingestion_month",
                                                                                             "ingestion_day"]
                                                                                         },
                                                                     format=op_format,
                                                                     transformation_ctx="glue_dynamic_write_df_" + str(
                                                                         subfolder_name))
    return glue_write_output
@pytest.fixture
def test_add_processing_date(read_dynamic_frame):
    add_processing_date_df = read_dynamic_frame.toDF().withColumn("ingestion_date",
                                                                  lit(datetime.date(datetime.today()))) \
        .withColumn("ingestion_year", lit(datetime.today().year)) \
        .withColumn("ingestion_month", lit(datetime.today().month)) \
        .withColumn("ingestion_day",
                    lit(datetime.today().day))
    return add_processing_date_df
@pytest.fixture
def test_calc_time():
    logged_time = time.strftime('%Y-%m-%d %H:%M:%S')
    return logged_time
logger.info(f'glue job {JOB_NAME} started at {test_calc_time()}')
try:
    test_create_source_dest_path_non_spi()
except TypeError as e:
    logger.info('Looks like no files/prefixes in non_spi prefix')
try:
    test_create_source_dest_path_spi()
except TypeError as e:
    logger.info('Looks like no files/prefixes in spi prefix')
    #  call to create list fucntion
configdata_rename = test_read_config_file(eui_opt_config_bucket, rename_key)
configdata_datatypechange = test_read_config_file(eui_opt_config_bucket, datatype_chng_key)
for subfolder in source_path:
    if subfolder.split("/")[1] == 'HHLD_NON_CR_DETL':  # to limit iteration  HHLD_NON_CR_DETL
        qualified_path_src = "s3://" + source_bucket + "/" + subfolder
        qualified_path_dst = "s3://" + dest_bucket + "/" + dest_dict[subfolder]
        glueJob.init(JOB_NAME, args)
        job_start_time = time.time()
        dy_read_input = test_glue_read(qualified_path_src, subfolder)
        record_count(dy_read_input.count(), qualified_path_src)
        if dy_read_input.count() == 0:
            continue
        stage_errors_count('create_dynamic_frame_from_options', dy_read_input.stageErrorsCount())
        # drop columnns from prod_ord table
        if subfolder.split("/")[1] == 'PROD_ORD':
            logger.info(f'prod_ord schema - {dy_read_input.printSchema()}')
            add_processing_date_dataframe = test_add_processing_date(test_drop_columns(dy_read_input, 'CASE_NUM', 'DO_NOT_USE'))
        else:
            add_processing_date_dataframe = test_add_processing_date(dy_read_input)
        #  new function  calls to read changes from json file to rename columns
        #  new function calls  to read changes from json file for datatype changes
        #  new function  calls to rename columns
        #  new function calls  to change datatype column
        #  subfolder  is like /eui/addr_info
        prefix = subfolder.split("/")[1]
        config_rename_prefix_lst = test_craete_flat_of_changes(configdata_rename, prefix, 'rename')
        config_datatype_prefix_lst = test_craete_flat_of_changes(configdata_datatypechange, prefix, 'datatype')
        renamed_df = add_processing_date_dataframe
        logger.info(f'***** transformations for {prefix} will be starting *****')
        for itrlist in config_rename_prefix_lst:
            renamed_df = test_update_data_type(renamed_df, itrlist, 'rename')
        datatype_renamed_df = renamed_df
        for itrlist in config_datatype_prefix_lst:
            datatype_renamed_df = test_update_data_type(datatype_renamed_df, itrlist, 'datatype')
        logger.info(datatype_renamed_df.printSchema())
        logger.info(f'*****transformations for {prefix} completed *****')
        dynamic_frame_output = DynamicFrame.fromDF(datatype_renamed_df, glueContext, "dynamic_frame_output")
        stage_errors_count('DataFrame_to_DynamicFrame', dynamic_frame_output.stageErrorsCount())
        logger.info(str(qualified_path_dst))
        glue_dynamic_write_df = test_glue_write(qualified_path_dst, subfolder)
        stage_errors_count('write_dynamic_frame_from_options', glue_dynamic_write_df.stageErrorsCount())
        job_end_time = time.time()
        glueJob.commit()
        logger.info(f' {job_end_time - job_start_time} seconds taken to complete {qualified_path_src} path')
logger.info(f'glue job {JOB_NAME} completed at {test_calc_time()}')
